\documentclass[11pt,letterpaper, twocolumn]{article}
\usepackage{graphicx} % Required for inserting images
\usepackage[latin1]{inputenc}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage{float}
\usepackage{fancyhdr}
%multi-row
\usepackage{multirow}
\usepackage[left=0.8in, right=0.8in, top=1in, bottom=1in, headsep=0pt
  ]{geometry}
\title{{\bf Image Processing and Classification Model for Screening Mammography in Automated Detection of Breast Cancer}}
\author{Vinayak Bassi, Youngmin Kim, Chalida Naiyaporn, Nawat Swatthong, Seonho Woo \\
vbassi, yngmkim, chalida, nawatsw, clairewo @umich.edu}
\date{}
\lhead{EECS545}
\rhead{Final Report}
\begin{document}

\maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Abstract}
% Breast cancer is well-known that it is hardly diagnosed at its early stage. Using image classification to identify its malignancy using machine learning methods will bring up a substantial effect on medical systems and the causality rate of patients. In this analysis, we applied the CNN (Convolutional Neural Network) and ViT (Visual Transformer) models, mostly CNN, to detect breast cancer from mammogram images and evaluate the model performances based on the preprocessing methods and steps. 

% Our preliminary results show that both models without data preprocessing perform poorly and are overfitting. In our next step of model development, data preprocessing will be integrated into the pipeline model and simultaneously combined with other types of cancer X-ray images into the original dataset to improve the classification performance of both ViT and CNN models.

Breast cancer is well-known that it is hardly diagnosed at its early stage, using image classification to identify its malignancy using machine learning methods will bring up a substantial effect on medical systems and the causality rate of patients. In this analysis, we applied the CNN (Convolutional Neural Network) and ViT (Visual Transformer) models, mostly CNN, to detect breast cancer from mammogram images and evaluate the model performances based on the preprocessing methods and steps. 

Since our preliminary results show that baseline models in both architectures without data preprocessing perform poorly, we investigated the best classification model by various combinations of adding image data preprocessing methods. According to experiments with 5-fold cross-validation, we found that the most composite setting with ROI cropping, thresholding, augmentation, and auxiliary variables predictions performs best in CNN (ResNeXt) architecture. In most experiments, we found that the imbalanced nature of the input data significantly affected the model performance in downgrading, especially in the baseline model and implementation of the test data. In the future, we would work on improving model robustness and performance by training ROI extraction on other mammography data and aggregating some ROI crop modules for better ROI crop patches.

\section{Introduction}
Breast cancer has long been recognized as a difficult disease to diagnose in its early stages. The World Health Organization has emphasized the importance of early detection, as cancer cells spread quickly, and the fatality rate increases dramatically as the disease progresses. Mammogram images are currently analyzed by highly-trained radiology experts to distinguish cancerous from non-cancerous tissue. However, the shortage of trained radiologists in many countries has created a pressing need for innovative solutions.

Machine learning techniques offer a promising approach to improving breast cancer detection in mammography. By supplementing the skills of human radiologists, these tools could help detect the disease in its earliest stages. This technology has the potential to improve patient outcomes while also increasing the efficiency of the healthcare system. By freeing up medical professionals' time and reducing costs, automated detection systems could enable healthcare providers to focus on other essential tasks and improve patient care. Overall, implementing machine learning techniques for breast cancer detection represents a promising opportunity to impact healthcare outcomes positively.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Works}
With the advent of machine learning and deep learning algorithms, diagnosing cancerous tumors and further treating them has become more efficient. We found research that has used Support Vector Machines (SVM) (Wang et al., 2018; Vijayarajeswari et al., 2019)${}^{[1][2]}$,  Naive Bayes Classifiers(Karabatak, 2015)${}^{[3]}$, and advanced deep learning algorithms (Convolutional Neural Networks, Vision Transformer) (Refat Khan et al.; Chen et al., 2022)${}^{[4][5]}$ to classify and detect breast cancer.
A balanced image dataset in terms of medical images has almost equal representations of both normal and abnormal cases. Much previous research that used neural network frameworks has used a balanced image dataset to classify benign and malignant mammography images. The article by Ragab Dina A. et al.${}^{[6]}$, which uses a deep convolutional neural network and SVM to classify breast cancer mammography images, has reported the use of the Curated Breast Imaging Subset of DDSM (CBIS-DDSM) which is a balanced dataset.

Generally, the available dataset for image mammograms is an imbalanced dataset. Imbalance datasets pose a significant challenge for developing deep learning models, especially CNN. We saw in some research work where they have used pre-trained CNN on a sample of DDSM database (an unbalanced mammograms dataset) (Tsochatzidis L. et al.)${}^{[7]}$. To address the issue of imbalance, researchers have explored various techniques, such as oversampling, undersampling, and data augmentation, to balance the datasets and improve the performance of CNNs. Additionally, transfer learning, where pre-trained CNNs on large datasets are fine-tuned for breast cancer detection, has also been used to address the imbalance in the datasets.

Our study used an imbalanced dataset provided by the Radiological Society of North America (RSNA) as part of a Kaggle competition. While referencing other submissions in the competition, we saw a strong emphasis on using medical preprocessing techniques such as ROI (Region-of-Interest) cropping and data augmentation techniques. In terms of deep learning models, many have reported the use of pre-trained CNN models fine-tuned for mammograms dataset. The winner of the competition has trained the YOLOx framework on external breast mammograms to generate an enhanced ROI crop and further train those images on a CNN architecture (4 x Convnextv1-small 2048x1024)${}^{[8]}$.

In our study, we have used importance sampling to manage the imbalanced dataset. We also deep-dived into techniques of data augmentation, thresholding, and auxiliary prediction. We have tried to learn cross-entropy auxiliary targets, which include image and auxiliary components (density, implant, BIRADS, etc.) while training which enhanced our prediction score. We also trained the RSNA dataset for a pre-trained ViT model.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Data}
%Description of Imbalanced Data
We use a dataset from the Kaggle competition, which originated from the Radiological Society of North America (RSNA), a non-profit organization that serves as a representative of 31 subspecialties related to radiology from 145 countries globally. The fact regarding 2\% of images contains cancer is both challenging and reflective of real-world situations where only a small number of people have cancer. 

\begin{figure}[H]
    \centering
    \includegraphics[scale = 0.6]{Final/Figures/L_malignant+R_benign.png}
    \caption{Malignant (left) and Benign (right) Mammograph Sample}
    \label{fig:malignant}
\end{figure}
As shown in Figure 1, it is hardly capable for humans to discriminate between malignant(left) and benign(right) scans.

\begin{figure*}[t]
    \centering
    \includegraphics[scale = 0.7]{Final/Figures/model diagram.png}
    \caption{Model Diagram}
    \label{fig:final_model_diagram}
\end{figure*}
The data for mammography breast images were collected from 11,913 patients. Each patient provided 2-side of the breast and at least two views of the orientation: the total number of images in this dataset was 54,706. Apart from the images, there are 9 variables that were collected from patients; age, whether the image was left or right (laterality), the orientation of the image (view: CC, MLO), whether the patient has breast implant (implant), a rating for density of breast tissue (density: A, B, C, D), whether the patient follow-up biopsy (biopsy), whether cancer proved to be an invasive (invasive), diagnosis for breast (BIRDAS: 0-the breast required follow-up, 1-the breast was rated as negative for cancer, and 2-the breast was rated as normal), and whether the case was unusual difficult (difficult\_negative\_case). For analysis, we make image data into training and test groups, split with a 4:1 ratio. 

\begin{table}[H]
\resizebox{\columnwidth}{!}{
\begin{tabular}{|c||c|c|c|}
    \hline
    Images\slash Datasets & Full & Training & Test \\ 
    \hline
    All & 54,760 & 43,805 & 10,901 \\
    Malignant (Cancer) & 1,158 & 917 & 231\\
    Benign (Non-cancer) & 53,548 & 42,878 & 10,670\\
    \hline
\end{tabular}}
\caption{Data sizes per groups}
\end{table}


\subsection{Features}
Since the total number of images was 54,706,  1,158 (2.12\%) images were cancer images. We can see that the images between cancer and non-cancer are imbalanced and might cause a problem in our analysis. So, we decided to deal with the imbalanced data by using the F1 score to evaluate the best model.
\begin{table}[h]
\resizebox{\columnwidth}{!}{
% \footnotesize
\begin{tabular}{|c||c|}
    \hline
    Variables & Median(IQR)\slash \% \\ 
    \hline
    Age & 59 (51, 66) \\ Laterality: Right & 50.16\% \\
    View: MLO & 51.01\% \\ Implant: No & 97.30\% \\
    Density: B & 23.13\% \\ Biopsy & 94.57\% \\
    Invasive & 98.50\% \\ Difficult Negative Case: No & 85.92\% \\
    BIRADS: required follow-up & 15.08\% \\ BIRADS: labeled as negative (Benign) & 28.83\% \\
    BIRADS: labeled as normal & 4.14\% \\ Cancer & 97.88\%\\
    \hline
\end{tabular}}
\caption{Median and Interquartile of features}
\end{table}

Table 2 above shows the median value for each variable in the data with its interquartile range (25\%, 75\%) for the numerical variable or the percent of data for the categorical variables.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{Final/Methods.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Result and Discussion}
In our study, we initially compared the performance of Convolutional Neural Network (CNN) and Visual Transformer (ViT) models for predicting breast cancer using mammography images. However, after testing several ViT models, we found that they did not perform well, which led us to focus on CNN models for our experiments. As shown in Table 3, we tested several methods to address the imbalance, and it turned out to have an actual improvement in model performance. 

The baseline model using the original image with only resizing demonstrates a poor performance of F1 score below 0.05 for both models in the evaluation set, which is probably due to imbalanced data.  However, as we add each step of ROI cropping, Threshold, Augmentation, and Auxiliary, gradual improvement in their performance is observed. We used 5-fold cross-validation in training data to determine the best classification model with multiple composite data preprocessing techniques tested in this experiment. 

In contrast to better performance in most composite model settings using evaluation data, as expected, model performance on all test data (i.e., F1 score) seems that they have off the trend that we observed that it is not gradually increased from the simplest model. This is also because of the data's nature, which is imbalanced substantially, which can be inferred from its initial F1 score of 0.027 of the model, tested on raw mammography images.

Remarkably, after adding auxiliary prediction, which involved predicting cancer as well as 11 variables (listed in Table 2), and calculating the loss in backward propagation, the model's performance is significantly improved. A reasonable guess on its dramatic increase in model performance is that it provided additional training on the imbalanced data.

As a result, the best result is accomplished by the CNN model with all data preprocessing steps, achieving an F1-score of 0.287, and we found that optimizing the classification threshold by cross-validation may have caused overfitting on the test dataset.

\subsection{Investigation of why ViT has a poor performance}
According to our study, we observed that the ViT model has poor performance with any preprocessing methods. We made some reasonable guesses on top of our results, which lack parameter search steps to optimize the model condition and find the optimal condition to perform the model. 

In addition, if the dataset contains small images or has limited variability in visual features, a ResNeXt CNN model might be able to capture these features effectively through the use of convolutional layers and skip connections, whereas the ViT DeiT model might not be able to fully utilize the self-attention mechanisms for learning such features. Moreover, ViT models require more computational resources and memory to process high-resolution images, while ResNeXt models can handle lower-resolution images with fewer computational resources.

\subsection{Examples of classified images: \\ correctly and incorrectly predicted}
As we see in Figure 4, the image on the left is correctly predicted as the possible reason that the visible tumor is condensed and has a clear boundary for the model to be classified. However, the tumor shape in the right-side image is more spread and has an obscure boundary that can be easily misinterpreted if the tumor is benign or malignant.

Below are the sample images that are predicted correctly and incorrectly in their malignancy: 
\begin{figure}[H]
    \centering
    % \includegraphics[scale = 0.4]{Final/Figures/Correctly_classify.png}
    % \caption{Well-Predicted Image Sample in classification}
    % \label{fig: classified_True}
    % \includegraphics[scale = 0.4]{Final/Figures/Incorrectly_classify.png}
    % \caption{Poorly-Predicted Image Sample in classification}
    % \label{fig: classified_False}
    \includegraphics[scale = 0.6]{Final/Figures/predicted_correct:incorrect.png}
    \caption{Well (left) and Poorly-Predicted (right) Image Sample in classification}
    \label{fig: classified_correct+incorrect}    
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
In conclusion, we compared the performance of several models for predicting breast cancer using mammography images and found that our model with the most comprehensive settings outperformed all others. However, we also observed that the dataset was substantially imbalanced in positive cancer cases, with positive cases comprising only 2\% of total images and a count of less than 1,000, which is a very small number compared to its sample size. This likely played a substantial role in the poor test prediction performance. 

The highest-performing Kaggle submission achieved an F1 score of 0.55, which is generally not considered a good performance, but this result partially justified our scores. The difference in the F1 score between the best Kaggle submission model and our model may have been caused by the size of the training data. Specifically, the Kaggle submission used the entire dataset with some external image data, whereas our model used only the data from splitting the dataset into train/test with no additional data to train the model. 

Furthermore, we found that most research in this field uses balanced data with about 10 to 50\% positive case images, which may be a factor in the differences between our results and those of other studies. However, in real-world data, it is more likely to observe fewer patients with cancer out of total observations, which makes our results reasonable given the nature of the data.

Overall, this study presents a meaningful approach for generating good results from imbalanced and small real-world datasets. We tested several models, including CNN and ViT, and found that the researchers' CNN (ResNext) model performed better than all tested ViT models. This was unexpected because ViT models are considered to be well-performed and state-of-the-art models in most other fields using regular image training models, which are similar to our models. Hence, we plan to investigate the hidden reasons behind this behavior and figure out the issue in future studies.

\section{Future Work}
\begin{figure*}[t]
    \centering
    \includegraphics[scale = 0.7]{Final/Figures/future_work.png}
    \caption{YOLOx Image Preprocessing}
\end{figure*}

In this study, we have studied methods to enhance the model and have observed that medical image preprocessing enables a higher prediction accuracy sometimes on simpler models, which is interesting. In future research, we would test some other methods to improve the model's robustness and performance: training ROI extraction (e.g., YOLOx model) on other mammography datasets and a fusion of ROI crop modules that identify optimal centers in the images, thus giving out pixel-level saliency maps and thus identifying better ROI crop patches.

\begin{center}
    \section*{Reference}
\end{center}
{[1]} Wang, H., Zheng, B., Yoon, S. W., & Ko, H. S. (2018). A support vector machine-based ensemble algorithm for breast cancer diagnosis. European Journal of Operational Research, 267(2), 687–699.\\ https://doi.org/10.1016/j.ejor.2017.12.001 \\
{[2]} Vijayarajeswari, R., Parthasarathy, P., Vivekanandan, S., & Basha, A. A. (2019). Classification of Mammogram for early detection of breast cancer using SVM classifier and Hough Transform. Measurement, 146, 800–805. \\https://doi.org/10.1016/j.measurement.2019.05.083\\
{[3]} Karabatak, M. (2015). A new classifier for breast cancer detection based on naive Bayesian. Measurement, 72, 32–36. https://doi.org/10.1016/j.measurement.2015.04.028
{[4]} Pathan, R. K., Alam, F. I., Yasmin, S., Hamd, Z. Y., Aljuaid, H., Khandaker, M. U., & Lau, S. L. (2022). Breast cancer classification by using multi-headed Convolutional Neural Network modeling. Healthcare, 10(12), 2367. \\
https://doi.org/10.3390/healthcare10122367\\
{[5]} Chen, X., Zhang, K., Abdoli, N., Gilley, P. W., Wang, X., Liu, H., Zheng, B., & Qiu, Y. (2022). Transformers improve breast cancer diagnosis from unregistered Multi-View Mammograms. Diagnostics, 12(7), 1549.\\ https://doi.org/10.3390/diagnostics12071549 \\
{[6]} Ragab DA, Sharkas M, Marshall S, Ren J. Breast cancer detection using deep convolutional neural networks and support vector machines. PeerJ. 2019 Jan 28;7:e6201. doi: 10.7717/peerj.6201. PMID: 30713814; PMCID: PMC6354665.\\
{[7]} Tsochatzidis, Lazaros, Lena Costaridou, and Ioannis Pratikakis. 2019. "Deep Learning for Breast Cancer Diagnosis from Mammograms—A Comparative Study" Journal of Imaging 5, no. 3: 37. https://doi.org/10.3390/jimaging5030037\\
{[8]} https://www.kaggle.com/competitions/rsna-breast-cancer-detection/discussion/392449
{[9]} C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1–9, 2015.

%\newpage
%\input{Final/appendix.tex}

\end{document}